This project explored Runway ML's latent space capabilities when the platform allowed for deep StyleGAN2 model customization.

## Concept

The idea was to train StyleGAN2 models on cinematic sources—transforming film language through machine learning. I chose *The Big Blue* (1988), Luc Besson's meditation on humans and the ocean depths.

## Process

I extracted over 17,000 frames from the film, capturing its distinctive underwater cinematography, Mediterranean light, and color palette. This dataset trained a StyleGAN2 model to understand and reinterpret the visual patterns unique to the film.

The output is a latent walk video—continuous interpolation through the model's learned space, creating an abstract sequence that distills *The Big Blue* into fluid, morphing imagery that never existed in the original film.

## Technical Details

- **Source Material:** 17,000+ frames from *The Big Blue*
- **Model:** StyleGAN2 trained on custom dataset
- **Platform:** Runway ML (latent space exploration)
- **Output:** Latent walk video
- **Technique:** Frame extraction → Model training → Latent traversal

## Artistic Exploration

The project investigates how machine learning creates new visual interpretations of cinematic work—not copying footage, but generating imagery that captures the "feeling" of the source. The latent walk reveals what the model learned about the film's visual DNA, creating an abstract visual poem from oceanic blues, diving sequences, and sun-drenched scenes.
